---
title: "Week 1: Introduction to Generative AI and Coding Assistants"
subtitle: "Applied Generative AI for AI Developers"
author: "Amit Arora"
format:
  revealjs:
    theme: custom.scss
    slide-number: true
    show-slide-number: print
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    code-copy: true
---

## Course Welcome

- **Course Title**: Applied Generative AI for AI Developers
- **Week 1 Focus**: Generative AI Landscape and AI-Powered Coding Tools
- **Key Objectives**:
  - Understand the breadth of generative AI technologies
  - Explore core generative model architectures
  - Introduction to AI coding assistants
  - Gain insights into practical applications

## What is Generative AI?

- **Definition**: AI systems that can create new content

>Generative AI can be thought of as a machine-learning model that is trained to create new data, rather than making a prediction about a specific dataset. A generative AI system is one that learns to generate more objects that look like the data it was trained on.

_Source: https://news.mit.edu/2023/explained-generative-ai-1109_

## What is Generative AI (contd.)?

- **Core Characteristics**:
  - Learning from existing data
  - Generating novel, contextually relevant outputs
  - Spanning multiple modalities (text, image, code, audio)

## Generative Model Landscape: Key Generative Model Architectures

1. **Language Models**
   - GPT (Generative Pre-trained Transformer)
   - BERT (Bidirectional Encoder Representations)

2. **Image Generation Models**
   - DALL-E
   - Stable Diffusion
   - Midjourney

3. **Multimodal Models**
   - Amazon Nova (Pro, Lite, Micro)
   - Anthropic Claude (3.5 Sonnet, Opus 4)
   - Meta Llama 3.3 and Llama 4
   - Google Gemini 2.0
   - OpenAI GPT-4o and o1
   - Mistral Large

# Transformer Architecture - Fundamental Concepts

## NLP prior to embeddings and transformers

- **Pre-Embedding NLP Representations: Bag of Words**
  - Discrete, non-contextual representation of text
  - Treats documents as unordered collections of words
  - Loses semantic meaning and word order
  - High dimensionality with sparse vectors
  - No understanding of word relationships


## Origins of Modern Embeddings
  - Word2Vec paper [Paper](Efficient Estimation of Word Representations in Vector Space), [Explainer video for word embeddings](https://www.youtube.com/watch?v=wgfSDrqYMJ4)
  - Breakthrough in sequence-to-sequence learning
  - Replaced previous RNN and LSTM architectures
  - Enabled dense, contextual word representations
  - Captured semantic relationships between words

## Core Components of Transformer Architecture

1. Introduced in "Attention Is All You Need" (Google, 2017). [Paper](https://arxiv.org/abs/1706.03762), [Explainer video](https://www.youtube.com/watch?v=iDulhoQ2pro) and also [this video](https://www.youtube.com/watch?v=5MaWmXwxFNQ)

1. **MUST READ**: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) 
1. Key Building Blocks

    1. **Self-Attention Mechanism**
       - Allows model to weigh importance of different parts of input
       - Captures contextual relationships dynamically
       - Enables parallel processing of entire sequences

    1. **Positional Encoding**
       - Adds location information to input embeddings
       - Crucial for understanding sequence order
       - Enables models to understand context beyond word positioning

## Detailed Self-Attention Mechanism

### How Self-Attention Works

- **Key Components**:
  - Query (Q)
  - Key (K)
  - Value (V)

- **Attention Calculation**:
  1. Generate Q, K, V matrices
  2. Compute attention scores
  3. Apply softmax
  4. Create weighted representation

## Transformer Architecture Visualization

### Encoder-Decoder Structure

- **Encoder**
  - Processes input sequence
  - Generates contextual representations
- **Decoder**
  - Generates output sequence
  - Uses encoder representations
- **Multi-Head Attention**
  - Multiple attention mechanisms in parallel
  - Captures different types of dependencies

## Transformer Advantages

### Why Transformers Changed Everything

- **Parallel Processing**
  - Unlike RNNs, can process entire sequences simultaneously
- **Long-Range Dependencies**
  - Effectively capture distant contextual relationships
- **Scalability**
  - Easily parallelizable
  - Supports massive model architectures

## Limitations and Challenges

### Transformer Architecture Considerations

- **Computational Complexity**
  - Quadratic complexity with sequence length
- **Memory Requirements**
  - Large models need significant computational resources
- **Potential Mitigation Strategies**
  - Flash Attention and Flash Attention 2
  - Sparse Attention
  - Mixture of Experts (MoE) architectures
  - Efficient Transformer variants
  - Model distillation techniques
  - Extended context windows (100K+ tokens now common)

## Practical Implications

### Transformers in Real-World Applications

- Natural Language Processing
- Machine Translation
- Code Generation
- Multimodal AI Systems
- Conversational AI

## References and Deep Dive Resources

### Recommended Learning Materials

1. **Foundational Papers**
   - "Efficient Estimation of Word Representations in Vector Space" (Mikolov et al., 2013)
     - Word2Vec: Pioneering word embedding techniques
   - "Attention Is All You Need" (Vaswani et al., 2017)
     - Original Transformer architecture paper

2. **Practical Implementation Resources**
   - Karpathy's nanoGPT
     - GitHub: https://github.com/karpathy/nanoGPT
     - Minimalist GPT implementation
     - Educational reference for transformer internals

3. **Video Explanations**
   - Andrej Karpathy's "LLMs in a Hurry" 
     - [video](https://www.youtube.com/watch?v=zjkBMFhNj_g): Comprehensive overview of LLM internals
   - 3Blue1Brown Transformer Visualization
     - [video](https://www.3blue1brown.com/lessons/attention): Intuitive mathematical explanation

4. **Online Resources**
   - [Hugging Face Transformer Documentation](https://huggingface.co/docs/transformers/en/index)
   - [Jay Alammar's "Illustrated Transformer"](https://jalammar.github.io/illustrated-transformer/)
   - [Stanford CS224N lectures on Transformers](https://www.youtube.com/watch?v=LWMzyfvuehA)
   - [Anthropic Claude Documentation](https://docs.anthropic.com/)
   - [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)


## Next Week Preview

### Week 2 Focus: In-Context Learning (ICL)

- Few-shot learning mechanisms
- Practical ICL implementation
- Advanced prompt engineering techniques


## AI-Powered Coding Assistants

### Introduction to Coding Assistants

- **Evolution of Development Tools**: From syntax highlighting to AI-powered code generation
- **Modern Coding Assistants**:
  - GitHub Copilot
  - Amazon Q Developer
  - Cursor
  - Claude Code (Anthropic)
  - Windsurf
  - Aider
  - Continue.dev
  - Tabnine

## How Coding Assistants Work

- **Underlying Technology**: Large Language Models trained on code
- **Key Capabilities**:
  - Code completion and suggestion
  - Natural language to code translation
  - Code explanation and documentation
  - Bug detection and fixing
  - Test generation

## Practical Applications

- **Accelerating Development**: Faster prototyping and iteration
- **Learning Tool**: Understanding new languages and frameworks
- **Code Quality**: Consistent patterns and best practices
- **Developer Productivity**: Focus on architecture and logic

## Coding Assistants Demo

*Live demonstration and hands-on examples*

- Setting up coding assistants in your IDE
- Using natural language prompts
- Context-aware code generation
- Best practices for working with AI assistants

## Agentic Coding: The Next Evolution

### From Assistants to Agents

- **Traditional Assistants**: Code completion, suggestions within IDE
- **Agentic Coding Tools**: Autonomous multi-step task execution
  - Claude Code: Terminal-based agentic coding
  - Cursor Agent: IDE-integrated autonomous coding
  - Aider: Git-aware AI pair programming
- **Key Capabilities**:
  - Read and understand entire codebases
  - Execute multi-file refactoring autonomously
  - Run tests and fix failures iteratively
  - Interact with tools (git, terminal, APIs)

## Reasoning Models

### A New Paradigm in AI

- **What are Reasoning Models?**
  - Models that "think" before responding
  - Extended inference time for complex problems
- **Examples**:
  - OpenAI o1 and o3
  - Anthropic Claude with extended thinking
  - DeepSeek-R1
- **Use Cases**:
  - Complex code generation and debugging
  - Multi-step problem solving
  - Mathematical and logical reasoning

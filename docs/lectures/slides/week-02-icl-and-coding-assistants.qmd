---
title: "Week 2: In-context learning (ICL) and AI Coding Assistants"
subtitle: "Applied Generative AI for AI Developers"
author: "Amit Arora"
format:
  revealjs:
    theme: custom.scss
    slide-number: true
    show-slide-number: print
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    code-copy: true
---

## What is In-Context Learning (ICL)?

- **Definition**: A paradigm where models learn to perform tasks by being provided examples or instructions directly in the input (context).
- **Impact**: Reduces or eliminates the need for fine-tuning on specific tasks.
- **Analogy**: Teaching by showing examples without altering the student's core knowledge.
- **Key Feature**: Enables models to generalize without retraining.

---

## Why Use In-Context Learning?

- **Advantages**:
  - No need for model retraining or additional compute-intensive fine-tuning.
  - Enables quick adaptation to novel tasks.
- **Applications**:
  - Rapid prototyping.
  - Tasks with small or specialized datasets.

---

## Mechanism of ICL

- **Few-Shot Approach**: The model is given examples of inputs and desired outputs as part of its prompt.
- **Mechanism**:
  - Uses the input examples to infer the pattern.
  - Applies this pattern to new inputs.
- **Key Insight**: Leverages the pre-trained knowledge of large language models.

---

## Hierarchy of Techniques

1. **Prompt Engineering**:
   - Easiest to implement.
   - Iterative improvement of task instructions and examples.
   - **When to use**: Small datasets, quick experiments.

2. **Fine-Tuning**:
   - Adjusts model weights for specific tasks.
   - Requires more resources (compute, labeled data).
   - **When to use**: High accuracy demands, domain-specific tasks.

3. **Continued Pre-Training (cPT)**:
   - Extends the pre-training phase with domain-specific data.
   - **When to use**: Large shifts in domain or task requirements.

---

## Pros and Cons of Each Technique

| Technique          | Pros                                | Cons                                   |
|--------------------|-------------------------------------|----------------------------------------|
| Prompt Engineering | Fast, low cost, no retraining.     | Limited accuracy, trial and error.    |
| Fine-Tuning        | Improved task performance.         | Requires labeled data, costly.        |
| cPT                | Handles major domain shifts well.  | Expensive, needs significant data.    |

---

## Prompt Engineering Overview

- **Zero-Shot Learning**:
  - No examples provided.
  - Example: "Summarize the following paragraph."

- **One-Shot Learning**:
  - One example provided.
  - Example: "Translate 'Bonjour' to English. Example: 'Hola -> Hello'. Translate 'Merci'."

- **Few-Shot Learning**:
  - Multiple examples provided.
  - Example: "Classify these reviews: 'I love this movie!' -> Positive, 'I hate this book.' -> Negative. Classify: 'This product is amazing.'"

---

## Zero-Shot Learning Example

- **Task**: Summarization.
- **Prompt**:
  - "Summarize the following text: 'Large language models are transforming AI.'"
- **Models**:
  - LLaMA3: Short and factual.
  - Claude: Polished and coherent.
  - Amazon Nova: Emphasizes conciseness.

---

## One-Shot Learning Example

- **Task**: Question Answering.
- **Prompt**:
  - "Question: Who developed the theory of relativity?
    Example: Question: Who invented the telephone? Answer: Alexander Graham Bell.
    Answer:"
- **Output**:
  - LLaMA3: Albert Einstein.
  - Claude: Einstein.
  - Amazon Nova: Albert Einstein.

---

## Few-Shot Learning Example

- **Task**: SQL Generation.
- **Prompt**:
  - "Examples:
     Input: Find all employees hired after 2020.
     Output: SELECT * FROM employees WHERE hire_date > '2020-01-01';
     Input: Count the number of departments.
     Output: SELECT COUNT(*) FROM departments;
     Input: List all customers from New York."
- **Output**:
  - LLaMA3: SELECT * FROM customers WHERE city = 'New York';
  - Claude: SELECT name FROM customers WHERE location = 'New York';
  - Amazon Nova: SELECT * FROM customers WHERE city = 'NYC';

---


## Anatomy of a Prompt

- **Structure**:
  - System Message: Defines the model's role or behavior.
  - User Message: Specifies the task or query.
  - Assistant Message (optional): Provides context or prior responses.
- **Examples**:
  - System: "You are a helpful assistant."
  - User: "Summarize the following text: 'Generative AI is a game changer.'"
  - Assistant: "Generative AI is transformative."
- **Key Idea**: Clear instructions improve response quality.

---

## The Messages API

- **Components**:
  - `system`: Sets the model's tone and scope.
  - `user`: Contains the main prompt or task.
  - `assistant`: Used for context in iterative tasks.
- **Flow**:
  - Messages are passed as a sequence.
  - Each message builds upon the previous ones.
- **Benefits**:
  - Enhances multi-turn interactions.
  - Allows dynamic context updates.


---

## Inference parameters

the Messages API allows you to interact with the model in a conversational way. You can define the role of the message and the content. The role can be either system, assistant, or user. The system role is used to provide context to the model, and the user role is used to ask questions or provide input to the model.

Users can get tailored responses for their use case using the following inference parameters while invoking foundation models:

  - `temperature` – Temperature is a value between 0–1, and it regulates the creativity of the model's responses. Use a lower temperature if you want more deterministic responses, and use a higher temperature if you want more creative or different responses from the model.
  - `top_k` – This is the number of most-likely candidates that the model considers for the next token. Choose a lower value to decrease the size of the pool and limit the options to more likely outputs. Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs.
  - `top_p` – Top-p is used to control the token choices made by the model during text generation. It works by considering only the most probable token options and ignoring the less probable ones, based on a specified probability threshold value (p). By setting the top-p value below 1.0, the model focuses on the most likely token choices, resulting in more stable and repetitive completions. This approach helps reduce the generation of unexpected or unlikely outputs, providing greater consistency and predictability in the generated text.

---

## Inference parameters (contd.)

  - `stop sequences` – This refers to the parameter to control the stopping sequence for the model’s response to a user query. For Meta-Llama models this value can either be "<|start_header_id|>", "<|end_header_id|>", or "<|eot_id|>".


---

## Prompting Best Practices

- **General Guidelines**:
  - Be specific: "Translate this sentence" vs. "Translate."
  - Avoid ambiguity: Use complete instructions.
  - Provide examples for complex tasks.

- **For GPT-4**:
  - Use detailed instructions for creative tasks.
  - Include constraints like word limits or tone.

- **For Claude**:
  - Prefers natural and conversational language.
  - Excels at reasoning tasks when examples are provided.

- **For LLaMA**:
  - Keep prompts concise.
  - Focus on factual and structured queries.

---

## Prompting Best Practices

| Model family | Prompt Engineering Reference |
|--------|---------------------------|
| Amazon Nova | https://docs.aws.amazon.com/nova/latest/userguide/prompting.html |
| Anthropic Claude | https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/ |
| Meta LLaMA | https://www.llama.com/docs/how-to-guides/prompting/ |

---

## Example: Claude Messages API with Amazon Bedrock

- **Task**: Summarize a document.

```python
import boto3

# Initialize the Bedrock client
bedrock = boto3.client('bedrock', region_name='us-west-2')

# Define messages in the API format
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Summarize the following text: 'Generative AI is transforming industries by automating creative tasks.'"}
]

# Call Claude via Bedrock
response = bedrock.invoke_model(
    modelId="anthropic.claude-v1",
    body={"input": {"messages": messages}}
)

# Print the response
print(response["output"])
```

---

## Advanced Example: Few-Shot Learning with Claude

- **Task**: Classify sentiment.

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant that classifies sentiment."},
    {"role": "user", "content": "Examples:\nReview: 'I love this product!' -> Positive\nReview: 'This is the worst service ever.' -> Negative\nReview: 'The delivery was on time.' ->"}
]

response = bedrock.invoke_model(
    modelId="anthropic.claude-v1",
    body={"input": {"messages": messages}}
)

print(response["output"])
```

---

## Multi-Modal In-Context Learning

- **Definition**: Extending ICL to handle tasks across multiple data types (text, images, audio).
- **Examples**:
  - Text-to-Image generation: Providing prompts with descriptive instructions.
  - Image Captioning: Few-shot examples pairing images with captions.
- **Future Applications**:
  - Interactive assistants.
  - Cross-domain insights.



---

## Prompt engineering - examples

Text to SQL prompt with Meta-LLaMA3

```{.bash}
messages = [
{
  "role": "system",
  "content":
    """You are a mysql query expert whose output is a valid sql query.
       Only use the following tables:
       It has the following schemas:
       <table_schemas>
       {table_schemas}
       <table_schemas>
       Always combine the database name and table name to build your queries. You must identify these two values before proving a valid SQL query.
       Please construct a valid SQL statement to answer the following the question, return only the mysql query in between <sql></sql>.
    """
},
{
  "role": "user",
  "content": "{question}"
}]
```

---

## Prompt engineering - examples - Few-shot prompting

```{.bash}
Extract the relevant information from the following parahrapgh and present it in a JSON format.

Michael Doe, a 45-year-old teacher from Boston, Massachusetts, is an avid reader and enjoys gardening during his spare time.
Example 1:
Paragraph: "John Doe is a 32-year-old software engineer from San Francisco, California. He enjoys hiking and playing guitar in his free time."
"employee": {
    "fullname": "John Doe",
    "city": "San Francisco",
    "state": "California",
    "occupation": "software engineer",
    "hobbies": ["hiking", "playing guitar"],
    "recentTravel": "not provided"
},
Example 2:
Paragraph: "Emily Jax, a 27-year-old marketing manager from New York City, loves traveling and trying new cuisines. She recently visited Paris and enjoyed the city's rich cultural heritage."
"employee": {
    "fullname": "Emily Jax",
    "city": "New York City",
    "state": "New York",
    "occupation": "marketing manager",
    "hobbies": ["traveling", "trying new cuisines"],
    "recentTravel": "Paris"
}
```
This produces the following output

```{.bashrc}
{
  "employee": {
    "fullname": "Michael Doe",
    "city": "Boston",
    "state": "Massachusetts",
    "occupation": "teacher",
    "hobbies": [ "reading", "gardening"],
    "recentTravel": "not provided"
  }
}
```

---

## Prompt engineering - examples - Task decomposition

```{.bashrc}

Break down the task of planning a vacation into smaller, manageable steps.
1. Choose a destination.
2. Set a budget.
3. Research accommodations.
4. Plan activities.
5. Book flights and accommodations.
6. Pack and prepare for the trip.
```

---

## Prompt engineering - examples - Chain of Thought

```{.bashrc}
Solve the following math problem step by step.

If you have 10 apples and you give 3 apples to your friend,
then buy 5 more apples, and finally eat 2 apples,
how many apples do you have left?
```

---

# From Prompt Engineering to Context Engineering

## The Evolution: Prompt Engineering is Dead?

### The 2025-2026 Paradigm Shift

- **Gartner (July 2025)**: "Context engineering is in, and prompt engineering is out"
- **The Problem with Prompts Alone**:
  - LLMs have finite context windows (4K-200K tokens)
  - Flooding with irrelevant instructions dilutes important information
  - Longer/trickier prompts yield diminishing returns
  - Prompts cannot compensate when model lacks situational data

### Key Insight

> "Prompt engineering is what you do **inside** the context window. Context engineering is how you decide **what fills** the window."

---

## What is Context Engineering?

### Definition (Andrej Karpathy, 2025)

> "Context engineering is the delicate art and science of filling the context window with just the right information for each step."

### It's a System-Level Discipline

- **Not just prompts** - Managing everything the model sees at runtime:
  - Retrieved documents (RAG)
  - System state and memory
  - Prior outputs and conversation history
  - Tool definitions and schemas
  - Results from external APIs
  - User preferences and permissions

---

## Why Context Engineering Emerged

### LLM Limitations Drive the Need

1. **LLMs are Stateless**
   - They don't "remember" unless you reinsert memory into context

2. **LLMs Hallucinate**
   - Contextual grounding through external data reduces this

3. **LLMs are Brittle**
   - Prompt-only approaches don't scale or generalize

### Industry Evidence (LangChain 2025 Report)

- 57% of organizations have AI agents in production
- 32% cite **quality** as top barrier
- Most failures traced to **poor context management**, not LLM capabilities

---

## Context Engineering: Core Components

### The Context Stack

1. **System Instructions** - Role, constraints, output format
2. **Retrieved Knowledge** - RAG, vector search results
3. **Memory** - Conversation history, user preferences
4. **Tool Definitions** - Available functions, API schemas
5. **Examples** - Few-shot demonstrations
6. **Current Task** - The actual user request

### Key Principle: Context is a Resource

- Treat context window like memory allocation
- Every token has a cost (latency, accuracy, price)
- Prioritize high-signal information
- Dynamically adjust based on task

---

## Context Engineering Best Practices

### Design Principles

1. **Relevance Filtering**: Only include what's needed for the current step
2. **Recency Weighting**: Recent context often more relevant
3. **Compression**: Summarize long histories
4. **Chunking**: Break large documents intelligently
5. **Caching**: Reuse computed context where possible

### Anti-Patterns to Avoid

- Stuffing everything into context "just in case"
- Static prompts that don't adapt to task
- Ignoring token limits until truncation occurs
- No separation between instruction and data

---

## Context Engineering Resources

### Essential Reading

- [Context Engineering: The Definitive 2025 Guide](https://www.flowhunt.io/blog/context-engineering/)
- [Context Engineering Guide - Prompting Guide](https://www.promptingguide.ai/guides/context-engineering-guide)
- [From Prompt Engineering to Context Engineering](https://www.ajeetraina.com/from-prompt-engineering-to-context-engineering/)
- [Context Engineering: A Complete Guide 2026](https://codeconductor.ai/blog/context-engineering/)
- [Context Engineering vs Prompt Engineering](https://dextralabs.com/blog/context-engineering-vs-prompt-engineering/)

### Key Figures

- **Tobi Lütke** (Shopify CEO): Popularized the term
- **Andrej Karpathy**: "Delicate art and science" definition

---

# AI Writing Its Own Prompts

## The DSPy Revolution

### What is DSPy?

- **Framework for programming—not prompting—language models** (Stanford NLP)
- Abstracts prompts into modular Python code
- **Automatically optimizes prompts** using ML techniques

### The Core Idea

Instead of:
```python
prompt = "You are a helpful assistant. Please summarize..."  # Manual trial & error
```

With DSPy:
```python
class Summarizer(dspy.Signature):
    """Summarize the given text concisely."""
    text = dspy.InputField()
    summary = dspy.OutputField()

# Optimizer finds the best prompt automatically!
```

---

## How DSPy Optimization Works

### The Process

1. **Define Behavior Declaratively** - Use Signatures to specify input/output
2. **Provide Training Examples** - Small set of examples with expected outputs
3. **Define a Metric** - How to measure success
4. **Run Optimizer** - AI automatically discovers optimal prompts

### Key Optimizers

| Optimizer | Approach |
|-----------|----------|
| **MIPROv2** | Bayesian optimization over instruction space |
| **COPRO** | Coordinate ascent hill-climbing |
| **SIMBA** | Self-reflective improvement from failures |
| **GEPA** | Trajectory reflection and gap analysis |

---

## DSPy: Real Results

### Automated Improvement

- One optimizer raised evaluation from **51.9% to 63.0%** automatically
- HotPot QA: **18.42% relative improvement** without manual prompt editing

### Why This Matters

- **Scales AI Development**: Automates the most time-consuming aspect
- **Model Switching**: Change from GPT-4 to Llama = config change + re-optimize
- **Reproducibility**: Systematic optimization vs. ad-hoc prompt tweaking

### Resources

- [DSPy Framework](https://dspy.ai/)
- [GitHub: stanfordnlp/dspy](https://github.com/stanfordnlp/dspy)
- [DSPy Tutorial 2025](https://www.pondhouse-data.com/blog/dspy-build-better-ai-systems-with-automated-prompt-optimization/)

---

## Modern Prompt Structures (2026)

### Beyond Simple Instructions

1. **XML-Tagged Sections** (Claude preferred)
```xml
<context>You are analyzing customer feedback</context>
<instructions>Classify sentiment and extract key themes</instructions>
<examples>...</examples>
<input>{{user_input}}</input>
```

2. **Structured Output Enforcement**
```
Respond in JSON format:
{"sentiment": "positive|negative|neutral", "themes": [...]}
```

3. **Chain-of-Thought Triggers**
```
Think step by step. Show your reasoning before the final answer.
```

---

## Advanced Prompting Techniques (2026)

### Self-Consistency

- Generate multiple responses with temperature > 0
- Take majority vote or aggregate answers
- Reduces hallucination on reasoning tasks

### Tree of Thoughts (ToT)

- Explore multiple reasoning paths
- Evaluate and prune branches
- Backtrack when stuck

### Reflection Prompting

```
First, provide your answer.
Then, critically evaluate your answer for errors.
Finally, provide your corrected final answer.
```

### Meta-Prompting

- Use one LLM call to generate/improve prompts for another
- "Write a prompt that would help an LLM solve this type of problem"

---

## The Future: Agentic Context Engineering

### ACE (Agentic Context Engineering)

- **Latest research direction** (2025-2026)
- Context evolves like a "playbook" that self-updates
- Models autonomously refine their prompts and memory

### Key Trends

1. **Dynamic Context**: Adapts in real-time based on task
2. **Self-Improving Systems**: Learn from failures automatically
3. **Multi-Agent Context Sharing**: Agents coordinate context
4. **Context as Code**: Version-controlled, testable, reviewable

### Industry Impact

- Enterprise AI spending: **$37B in 2025** (up from $11.5B in 2024)
- Organizations with robust context architectures see:
  - 50% improvement in response times
  - 40% higher quality outputs

---

## AI Coding Assistants: The Landscape

- **Evolution**: From OpenAI Codex & GitHub Copilot (autocomplete) to autonomous agents.
- **Models**: Powered by GPT-4, Claude 3.5 Sonnet, Gemini 1.5 Pro.
- **Main Categories**:
  - **Plugin-based**: Extensions for existing IDEs (VS Code, JetBrains).
  - **Agentic IDEs**: Standalone editors reimagined for AI collaboration.

---

## Category 1: Plugin-based Assistants

- **Philosophy**: Enhance your existing environment (VS Code, IntelliJ, Terminal).
- **Examples**:
  - **GitHub Copilot**: The industry standard for autocomplete & chat.
  - **Aider**: A CLI tool for pair programming; edits files directly via git.
  - **Roo Code**: VS Code extension enabling autonomous task execution.
  - **Claude Code**: Agentic CLI tool from Anthropic (often used alongside IDEs).

---

## Category 2: Agentic IDEs

- **Philosophy**: Deeply integrated AI that "sees" the whole codebase and can drive the editor.
- **Examples**:
  - **Cursor**: A VS Code fork. Famous for "Composer" (multi-file edits) and "Tab" (prediction).
  - **Windsurf**: By Codeium. Features "Cascade" flow and deep context awareness.
  - **Antigravity**: Advanced agentic coding environment (Google).

---

## Comparative Analysis

| Tool | Type | Key Strength | Link |
|------|------|--------------|------|
| **Cursor** | Agentic IDE | Best-in-class UI/UX, 'Composer' mode. | [cursor.com](https://cursor.com) |
| **Windsurf** | Agentic IDE | Deep context 'Flow', 'Cascade' agent. | [codeium.com/windsurf](https://codeium.com/windsurf) |
| **Antigravity** | Agentic IDE | Multi-agent orchestration, **free** for individuals. | [antigravity.google](https://antigravity.google) |
| **Claude Code** | CLI / Plugin | Research-grade agentic capabilities. | [anthropic.com](https://anthropic.com) |
| **Aider** | CLI Tool | Best for terminal users, git-aware. | [aider.chat](https://aider.chat) |
| **Roo Code** | Plugin | Open-source, highly configurable agent. | [roocode.com](https://roocode.com) |

---

## Spotlight: Google Antigravity

- **What is it?**: An agent-first IDE from Google DeepMind (announced Nov 2025).
- **Pricing**: **Free** for individuals with a personal Gmail account (public preview).
  - Access to Gemini 3 Pro/Flash, Claude Sonnet/Opus 4.5, GPT-OSS-120b.
  - Unlimited tab completions and generous weekly rate limits.
- **Key Differentiator**: Multi-agent orchestration via "Agent Manager".

---

## Antigravity: Core Capabilities

- **Agent-First Paradigm**: AI agents autonomously plan, execute, validate, and iterate.
- **Multi-Agent Orchestration**: Spawn multiple agent threads working in parallel (e.g., refactor + test).
- **Comprehensive Interaction**: Agents control editor, terminal, and browser.
- **Artifacts for Trust**: Produces rich markdown, diagrams, browser recordings, and diffs for human review.
- **VS Code Fork**: Familiar interface with an agent sidebar.

---

## MCP Integration with AI Coding Assistants

- **What is MCP?**: Model Context Protocol (Anthropic) — an open standard for connecting LLMs to external data and tools.
- **Why it Matters**: Transforms AI from static knowledge to dynamic agents with real-time context.
- **Supported Tools**:
  - **Claude Code**: Native MCP support for databases, APIs, and custom tools.
  - **Cursor**: Configure MCP servers in settings to connect to external systems.
  - **Antigravity**: Integrates with Google Workspace, GitHub, Slack via MCP-like connectors.

---

## MCP Use Cases in Development

- **Database Queries**: AI can directly query your Postgres/MySQL via MCP servers (e.g., **Supabase MCP**, **Firebase MCP**).
- **API Integration**: Connect to REST APIs, Slack, Notion, or GitHub for richer context (**GitHub MCP**, **Notion MCP**).
- **Documentation Access**: **Context7 MCP** provides up-to-date, version-specific docs for ~20,000 libraries (prevents hallucinations).
- **Design-to-Code**: **Figma MCP** extracts design context and generates code from Figma files.
- **Browser Automation**: **Playwright MCP** and **Chrome DevTools MCP** enable AI to control browsers for testing and debugging.
- **Custom Tools**: Build your own MCP servers to expose proprietary data or internal tools.

---

## Example MCP Workflow

- **Scenario**: Developer asks: "Find all users who signed up last week."
- **Steps**:
  1. Claude Code (via **Supabase MCP**) queries the database and returns results.
  2. Developer: "Generate a CSV export."
  3. Agent writes the script and executes it.
- **Benefit**: No context switching between IDE, database UI, and documentation.

---

## Spotlight: Claude Code & Configuration

- **What is it?**: A CLI-based agent that can traverse files, plan, and execute.
- **Configuring the Agent** (Files in your repo):
  - `claude.md`: **Project Context**. High-level architecture, style guides, and rules.
  - `skills.md`: **Tool Definitions**. Custom scripts/commands the agent can run.
  - `agent.md`: **Persona**. Defining the agent's role (e.g., "Senior Backend Engineer").
- **Versatility**: These configuration patterns apply to many agentic tools, not just Claude.

---

## Technique: Spec-Driven Development

- **The Problem**: Vague prompts lead to hallucinated or poor code.
- **The Fix**: **Plan First / Code Later**.
- **Process**:
  1. **Draft a Spec**: Create a markdown file describing requirements (API, Data Models).
  2. **Human Review**: You review the design.
  3. **Agent Reflection**: Ask the LLM: *"Review this spec for edge cases or security flaws."*
  4. **Execute**: Only generate code once the plan is solid.

---

## Technique: Team Persona

- **Concept**: Simulate a full engineering team.
- **Prompting Strategy**:
  - "Act as a **Product Manager**: Are requirements clear?"
  - "Act as a **Frontend Engineer**: Is the UI UX-friendly?"
  - "Act as a **Backend Engineer**: Is the DB schema normalized?"
  - "Act as **Infra/SRE**: How do we deploy this?"
- **Benefit**: Catch issues across different concerns before writing code.

---

## References and Further Reading

- **Code samples**:
  - https://github.com/NirDiamant/Prompt_Engineering/tree/main

- **Important Papers**:
  - Brown et al. (2020). "Language Models are Few-Shot Learners." [[GPT-3 Paper](https://arxiv.org/pdf/2005.14165)]
  - Wei et al. (2022). "Chain of Thought Prompting Elicits Reasoning in Large Language Models." [link](https://arxiv.org/pdf/2201.11903)
  - Min et al. (2022). "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?" [link](https://arxiv.org/pdf/2202.12837)


- **Comparison Studies**:
  - Fine-Tuning vs. In-Context Learning vs. cPT: "Prompting vs. Tuning: Which Approach Works Better for LLMs?" [Reddit thread](https://www.reddit.com/r/LocalLLaMA/comments/152s9ei/when_is_it_best_to_use_prompt_engineering_vs/)
  - Touvron et al. (2023). "LLaMA: Open and Efficient Foundation Language Models." [link](https://arxiv.org/pdf/2302.13971)
  - Ruoss, Pardo et al (2024) "LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations" [link](https://arxiv.org/html/2412.01441v1)


